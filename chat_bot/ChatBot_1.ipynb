{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tflearn\n",
    "import numpy as np\n",
    "#import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize stemmer, stopwords and clean sentence method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "#stemmer = LancasterStemmer()\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "only_alnum = re.compile(r\"[^\\w]+\") ## \\w => unicode alphabet\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = re.sub(only_alnum, \" \", sentence).strip()\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    #words = [stemmer.stem(w.lower()) for w in words if w not in stopwords]\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the documents from the intents.json, clean the sentences and add then to a list of documents, along with their respective tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 documents\n",
      "9 classes ['greeting', 'goodbye', 'thanks', 'hours', 'mopeds', 'payments', 'opentoday', 'rental', 'today']\n",
      "37 unique stemmed words {'hours', 'open', 'take', 'Good', 'mopeds', 'Thank', 'Thanks', 'work', 'How', 'rent', 'helpful', 'day', 'Hello', 'like', 'accept', 'See', 'When', 'cards', 'What', 'cash', 'That', 'today', 'Mastercard', 'I', 'Bye', 'Are', 'kinds', 'credit', 'Hi', 'Do', 'Is', 'Which', 'moped', 'anyone', 'Goodbye', 'later', 'Can'}\n",
      "[(['Hi'], 'greeting'), (['How'], 'greeting'), (['Is', 'anyone'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank'], 'thanks'), (['That', 'helpful'], 'thanks'), (['What', 'hours', 'open'], 'hours'), (['What', 'hours'], 'hours'), (['When', 'open'], 'hours'), (['Which', 'mopeds'], 'mopeds'), (['What', 'kinds', 'mopeds'], 'mopeds'), (['What', 'rent'], 'mopeds'), (['Do', 'take', 'credit', 'cards'], 'payments'), (['Do', 'accept', 'Mastercard'], 'payments'), (['Are', 'cash'], 'payments'), (['Are', 'open', 'today'], 'opentoday'), (['When', 'open', 'today'], 'opentoday'), (['What', 'hours', 'today'], 'opentoday'), (['Can', 'rent', 'moped'], 'rental'), (['I', 'like', 'rent', 'moped'], 'rental'), (['How', 'work'], 'rental'), (['today'], 'today')]\n"
     ]
    }
   ],
   "source": [
    "#import the intent file\n",
    "import json\n",
    "intents = []\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "classes = []\n",
    "dictionary = []\n",
    "documents = []\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #for each pattern, get the list of words\n",
    "        words = clean_sentence(pattern)\n",
    "        tag = intent['tag']\n",
    "        #get the unique set of classes\n",
    "        if tag not in classes:\n",
    "            classes.append(tag)\n",
    "        #tag each document with its class\n",
    "        documents.append((words, tag))\n",
    "        #unique set of stemmed words\n",
    "        dictionary.extend(words)\n",
    "\n",
    "dictionary = set(dictionary)\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(dictionary), \"unique stemmed words\", dictionary)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use googles precomputed word2vec mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use googles word vector model\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging\n",
    "\n",
    "#use the existing google bin to get word2vec representation\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../Datasets/GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create own word 2 vec\n",
    "#perform word2vec on these words\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "#build a word2vec representation using the given docs\n",
    "#wp = word2vec.Word2Vec(docs, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all the words into word2vec representation\n",
    "compute sentence2vec = summation of word2vec representations of all the words in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#converting documents into word2vec representations\n",
    "def convert_sentence2vec(documents, isUseZeros, isUseOnes):\n",
    "    missing_word_vecs={}\n",
    "    number_of_docs = len(documents)\n",
    "    word2vec_rep = np.zeros((number_of_docs, num_features))\n",
    "    count = 0 \n",
    "    labels = []\n",
    "    i=0\n",
    "    for document in documents:\n",
    "        tag = document[1]\n",
    "        doc_words = document[0]\n",
    "        #print(doc_words)\n",
    "        for word in doc_words: \n",
    "            try:\n",
    "                word2vec_rep[i]+=model[word]\n",
    "            except:\n",
    "                '''The word isn't in our pretrained word-vectors, hence we add a random gaussian noise\n",
    "                    to account for this. We store the random vector we assigned to the word, and reuse \n",
    "                    the same vector during test time to ensure consistency.'''\n",
    "                if word  not in missing_word_vecs.keys():\n",
    "                    if isUseZeros:\n",
    "                        missing_word_vecs[word] = np.zeros((num_features))\n",
    "                    elif isUseOnes:\n",
    "                        missing_word_vecs[word] = np.ones((num_features))\n",
    "                    else:\n",
    "                        missing_word_vecs[word] = np.random.normal(-0.25, 0.25, num_features)\n",
    "                word2vec_rep[i]+=missing_word_vecs[word]\n",
    "                count +=1\n",
    "        labels.append(tag)\n",
    "        i+=1\n",
    "    return word2vec_rep, labels, missing_word_vecs\n",
    "\n",
    "sent2vec_rep, labels, missing_word_vecs = convert_sentence2vec(documents, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 300)\n",
      "['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks', 'hours', 'hours', 'hours', 'mopeds', 'mopeds', 'mopeds', 'payments', 'payments', 'payments', 'opentoday', 'opentoday', 'opentoday', 'rental', 'rental', 'rental', 'today']\n"
     ]
    }
   ],
   "source": [
    "print(sent2vec_rep.shape)\n",
    "print(labels)\n",
    "#print(model['I']+ model['like']+model['rent']+model['moped'])\n",
    "#print(sent2vec_rep[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the word2vec_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train this vectorial representation (multi class classification problem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
