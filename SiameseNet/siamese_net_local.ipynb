{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sY8HfxCh_CSO"
   },
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcLZbnv4Ba3a"
   },
   "source": [
    "The objective of this experiment is to understand Siamese Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuObTdeFBiGY"
   },
   "source": [
    "Tons of data area available on the web (wikipedia, Google, Twitter, YouTube) that could be used to train an ML model.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "One such source is Google Images. You enter a text query and Google Images shows thousands of related images based on the query and text that are present on the web page with the related image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5Gw25muBlKU"
   },
   "source": [
    "In this experiment we would crawl images from Google Images and try to use this as data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHi5xGcCBnA-"
   },
   "source": [
    "1. Your task is to search for face images for 'AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha'\n",
    "\n",
    "2. Refine your search to faces (Google Images -> enter query -> Tools -> Type -> Face). You could also use movies', ads' names as additional query (e.g., \"Aamir 3 idiots\", \"Boman Irani Khosla Ka Ghosla\", \"Katrina Slice ad\" etc.). The results are noisy but they are useful, and moreover, they are avaible in abundance and for free!\n",
    "\n",
    "    a. Example: https://www.google.co.in/search?client=firefox-b-ab&dcr=0&biw=1366&bih=628&tbs=itp%3Aface&tbm=isch&sa=1&ei=5gbIWtCjN4n2vgSCoqzYBw&q=biswa+kalyan+rath\n",
    "\n",
    "3. Then use a browser extensions to download all the results into a directory. In this way you, would get around 300-600 images for each class. Overall, you should collect atleast 10000 images.\n",
    "    \n",
    "    a. Firefox: https://addons.mozilla.org/en-US/firefox/addon/google-images-downloader/\n",
    "    \n",
    "    b. Chrome: https://chrome.google.com/webstore/detail/download-all-images/ifipmflagepipjokmbdecpmjbibjnakm/related?hl=en\n",
    "    \n",
    "4. **Without cleaning** use these images as your training data. Test you results on IMFDB test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbKgGvN8BtWG"
   },
   "source": [
    "#### Run the Notebook on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFg6Jf8VBuTw"
   },
   "source": [
    "#### Authenticating Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VdAdf8O8O39S"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
    "!dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
    "!apt-get install -f\n",
    "!apt-get -y install -qq fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhoMm0ENB04C"
   },
   "source": [
    "#### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "IXfGhT9XFySf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-30 22:11:00--  https://www.dropbox.com/s/0b4txuf4si3659z/One_shot_Face_recognition.zip?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.1\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.1|:443... connected.\n",
      "OpenSSL: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol\n",
      "Unable to establish SSL connection.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/0b4txuf4si3659z/One_shot_Face_recognition.zip?dl=1\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNCSkMclB-jU"
   },
   "source": [
    "#### Unziping the downloaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "XqxnpOZkjwMT"
   },
   "outputs": [],
   "source": [
    "!unzip -qq -o One_shot_Face_recognition.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_p4CBA5VCEKW"
   },
   "source": [
    "#### Installing Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "riLJW9msg5oy"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6bXbVJpCKBq"
   },
   "source": [
    "#### Changing the Working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "aAhuoS_ioZ9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tgupta2/Documents/Training_Courses/AIML/Session10/One_shot_Face_recognition\n"
     ]
    }
   ],
   "source": [
    "%cd One_shot_Face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aq9clcuMCRVy"
   },
   "source": [
    "#### Importing the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "APSGlS_s_CSU"
   },
   "outputs": [],
   "source": [
    "# Importing pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "# Importing config.py file\n",
    "import config as cf\n",
    "from utils import *\n",
    "from light_cnn import LightCNN_9Layers #, LightCNN_29Layers, LightCNN_29Layers_v2\n",
    "#from resnet import resnet18\n",
    "from siamese_data_loader import *\n",
    "from contrastive import *   ### implementation of contrastive loss\n",
    "## Importing python packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\n"
     ]
    }
   ],
   "source": [
    "print(cf.data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oia-EoIHCjo0"
   },
   "source": [
    "#### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "LWG1XpBV_CSi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 1095\n",
      "5000 1095\n"
     ]
    }
   ],
   "source": [
    "img_root = cf.data_dir+'IMFDB_final/'\n",
    "\n",
    "train_list_file = cf.data_dir+'IMFDB_train_sorted.txt'   #### 5000 images for training\n",
    "val_list_file = cf.data_dirimg_root = cf.data_dir+'IMFDB_final/'\n",
    "\n",
    "train_list_file = cf.data_dir+'IMFDB_train_sorted.txt'   #### 5000 images for training\n",
    "val_list_file = cf.data_dir+'IMFDB_test_sorted.txt'      #### 1095 images for validation\n",
    "\n",
    "\n",
    "train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n",
    "val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n",
    "\n",
    "print(len(train_image_list), len(val_image_list))\n",
    "\n",
    "### Notice a new data loader for siamese networks. This gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n",
    "### see siamese_data_loader.py for details\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n",
    "                                                             resize = True, resize_shape=[128,128]), \n",
    "batch_size=32, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=10, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "\n",
    "#classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']+'IMFDB_test_sorted.txt'      #### 1095 images for validation\n",
    "\n",
    "\n",
    "train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n",
    "val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n",
    "\n",
    "print(len(train_image_list), len(val_image_list))\n",
    "\n",
    "### Notice a new data loader for siamese networks. This gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n",
    "### see siamese_data_loader.py for details\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n",
    "                                                             resize = True, resize_shape=[128,128]), \n",
    "batch_size=20, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=5, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "\n",
    "classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2Aee8StCokK"
   },
   "source": [
    "#### Command to check whether GPU is enabled or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "e0_YCzNqeLyB"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VvlieheJ_CSy"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Intilizaing the loss value as high value\n",
    "best_loss = 99999999\n",
    "\n",
    "num_classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y4NzPR-weLyH"
   },
   "outputs": [],
   "source": [
    "from torchvision import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "40tFVFoWeLyM"
   },
   "outputs": [],
   "source": [
    "feature_net = LightCNN_9Layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "ort2nuVUeLyS"
   },
   "outputs": [],
   "source": [
    "#feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth', map_location='cpu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "yUJKM-V5_CS-"
   },
   "outputs": [],
   "source": [
    "#deep feature net with 9 layers, we will train this\n",
    "feature_net = LightCNN_9Layers()   ### creates an object of this network architecture\n",
    "#feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth', map_location='cpu' )\n",
    "\n",
    "\n",
    "#layers_to_remove = ['fc2']\n",
    "#for layers_ in layers_to_remove:        \n",
    "#    del(feature_net._modules[layers_])\n",
    "    \n",
    "classifier = nn.Sequential(nn.Linear(256, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
    "                           nn.Linear(32, num_classes))\n",
    "\n",
    "feature_net.fc2 = nn.Sequential(nn.Linear(256, 16))\n",
    "feature_net = feature_net.to(device)\n",
    "classifier =  classifier.to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Dvh9E5z0_CTG"
   },
   "outputs": [],
   "source": [
    "### Intiliazing the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "siamese_loss = contrastive_loss()   ### Notice a new loss. contrastive.py shows how to compute contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JFloT0WAeLyw"
   },
   "outputs": [],
   "source": [
    "criterion = criterion.to(device)\n",
    "siamese_loss = siamese_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOmg3Ypb_CTO"
   },
   "source": [
    "#### Lets train the siamese networks. The objective is images from same class (+ pair, label = 0) should have similar feature and images from different classes (- pair, label = 1) should have different features. Instead of having two physical networks sharing the weights, in implementation we have only one network and first pass image_1 (to get its feature) and then pass image_2 (to get its feature) through the same network. We then compute the contrastive loss on these feature pairs from input image pairs. This saves a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "isK--8bS_CTS"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    feature_net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 1\n",
    "    #the trainloader gives 3 things, 2 images and a label whether they are similar or not\n",
    "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(trainloader):\n",
    "        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n",
    "        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n",
    "        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n",
    "        \n",
    "        loss = siamese_loss(features_1, features_2, targets.float())   ### compute the contrastive loss, computes the similarity between the features.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #print(1)\n",
    "        \n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f '% (train_loss/(batch_idx+1)))\n",
    "        \n",
    "    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n",
    "        #print(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qj7vLS23C_SS"
   },
   "source": [
    "#### Function to test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zMwHtczA_CTc"
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_loss\n",
    "    feature_net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 1\n",
    "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n",
    "        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #inputs_1, inputs_2, targets = Variable(inputs_1), Variable(inputs_2), Variable(targets)\n",
    "        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n",
    "        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2      \n",
    "        \n",
    "        loss = siamese_loss(features_1, features_2, targets.float())\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f '\n",
    "                         % (test_loss/(batch_idx+1)))\n",
    "        \n",
    "    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    losss = test_loss/len(testloader)\n",
    "    if  losss < best_loss:   ### save model with the best loss so far\n",
    "        print('Saving..') \n",
    "        state = {\n",
    "            'net': feature_net\n",
    "        }\n",
    "        if not os.path.isdir(cf.data_dir+'checkpoint'):\n",
    "            os.mkdir(cf.data_dir+'checkpoint')\n",
    "        torch.save(state, cf.data_dir+'checkpoint/siamese_ckpt.t7')\n",
    "        best_loss = losss\n",
    "    \n",
    "    return test_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3k0q1BOVDD86"
   },
   "source": [
    "#### Creating the files to store train and validation data loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KhnUSGoC_CTk"
   },
   "outputs": [],
   "source": [
    "experiment = 'siamese_IMFDB/'\n",
    "train_loss_file = open(cf.data_dir+experiment+\"train_loss.txt\", \"w\")\n",
    "val_loss_file = open(cf.data_dir+experiment+\"val_loss.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xKSAHBYR_CT0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [========================>..........] | Loss: 448.975                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 176/250 \r"
     ]
    }
   ],
   "source": [
    "feature_net = feature_net.to(device)\n",
    "optimizer = optim.Adam(feature_net.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n",
    "for epoch in range(0, 10):\n",
    "    train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    scheduler.step(test_loss)\n",
    "    print(\"Test Loss: \", test_loss)\n",
    "train_loss_file.close()\n",
    "val_loss_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xYMrwcsA_CUG"
   },
   "outputs": [],
   "source": [
    "### After training we load the model that performed the best on validation data (avoid picking overfitted model)\n",
    "### we will use the base pre-trained network for feature extraction only. This feature is used to train an MLP classifier.\n",
    "\n",
    "feature_net = torch.load(cf.data_dir+'checkpoint/siamese_ckpt.t7',map_location='cpu' )['net'].eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48su1H5b_CUO"
   },
   "source": [
    "#### Lets see how well does the siamese detect an imposter. We check whether image_2 is same individual as image_1 or an imposter. We do this by computing dissimilarity score between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kz9y72aY1HCv"
   },
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "lab = ['same', 'imposter']\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n",
    "        if batch_idx%10 == 0 or int(targets)==0:      ### show every tenth image or if its the same individual\n",
    "\n",
    "            inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n",
    "            features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n",
    "            features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n",
    "\n",
    "            dissimilarity = torch.nn.functional.cosine_similarity(features_1, features_2).item()\n",
    "            img = np.concatenate((inputs_1.data.cpu().numpy()[0][0], inputs_2.data.cpu().numpy()[0][0]), axis = 1)\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.text(100,20,str(dissimilarity), fontsize=24, color='r')     ### similarity score\n",
    "            plt.text(100,40,lab[int(targets.data[0])], fontsize=24, color='r')   ### ground truth\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9di4sNM_CUc"
   },
   "source": [
    "### Now we use this network for feature extraction and train an MLP classifier. Feature_net is not updated/train/tweak after this. We only train the MLP classifier.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BJTaxxb21riY"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files \n",
    "#uploaded = files.upload()\n",
    "\n",
    "import collections\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import cv2\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from random import shuffle\n",
    "import os.path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class custom_data_loader(data.Dataset):    ### custom data loader\n",
    "    \n",
    "    def __init__(self, img_root, image_list, mirror = True, crop = True, crop_shape = [256, 256],\n",
    "                 resize=False, resize_shape=[128,128], split = 'train', classes = 'IMFDB'):\n",
    "\n",
    "        self.img_root = img_root\n",
    "        self.split = split\n",
    "        \n",
    "        self.image_list = [line.rstrip('\\n') for line in open(image_list)]\n",
    "        \n",
    "        self.classes = None\n",
    "        if classes == 'IIC':\n",
    "            self.classes = ['baba_ramdev', 'biswa',  'dhinchak_pooja',  'khali',  'priya_prakash']\n",
    "        elif classes == 'IMFDB':\n",
    "            self.classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']\n",
    "\n",
    "        self.mirror = mirror\n",
    "        self.crop = crop\n",
    "        self.crop_shape = crop_shape\n",
    "        \n",
    "        self.resize = resize\n",
    "        self.resize_shape = resize_shape\n",
    "\n",
    "        #self.mean_bgr = np.array([123.68, 116.779, 103.939])   ### mean BGR of ImageNet\n",
    "        self.mean_bgr = np.array([0])   ### lightnet does not perform mean subtraction of input\n",
    "\n",
    "        self.files = collections.defaultdict(list)\n",
    "        for f in self.image_list:\n",
    "            self.files[self.split].append({'img': img_root+f, 'lbl': 0})\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files[self.split])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image_file_name = self.img_root + self.image_list[index]\n",
    "        \n",
    "        image = None\n",
    "        if os.path.isfile(image_file_name):\n",
    "            image = cv2.imread(image_file_name, 0)  ### 0 indicates load image as grayscale image\n",
    "        if image is None:\n",
    "            print('creating a default image')\n",
    "            image = np.zeros((self.resize_shape[1], self.resize_shape[0]))\n",
    "            if self.resize :\n",
    "                image = cv2.resize(image, (self.resize_shape[1], self.resize_shape[0]))   ### resize_shape is in [rows, cols]\n",
    "            \n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "            \n",
    "            if self.mirror:\n",
    "                flip = np.random.choice(2)*2-1    ### randomly flip the image\n",
    "                image = image[:, ::flip, :]\n",
    "\n",
    "            if self.crop:\n",
    "                image = self.get_random_crop(image, self.crop_shape)\n",
    "\n",
    "            label = [self.classes.index(i) for i in self.classes if self.image_list[index].split('/')[0] == i][0]\n",
    "        \n",
    "            l = torch.IntTensor(1, 1)\n",
    "            l = label\n",
    "\n",
    "            return self.transform_image(image), l\n",
    "            \n",
    "        else :    \n",
    "            if self.resize :\n",
    "                print('image found resizing')\n",
    "                image = cv2.resize(image, (self.resize_shape[1], self.resize_shape[0]))   ### resize_shape is in [rows, cols]\n",
    "            \n",
    "                image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "            \n",
    "            if self.mirror:\n",
    "                flip = np.random.choice(2)*2-1    ### randomly flip the image\n",
    "                image = image[:, ::flip, :]\n",
    "\n",
    "            if self.crop:\n",
    "                image = self.get_random_crop(image, self.crop_shape)\n",
    "\n",
    "            label = [self.classes.index(i) for i in self.classes if self.image_list[index].split('/')[0] == i][0]\n",
    "        \n",
    "            l = torch.IntTensor(1, 1)\n",
    "            l = label\n",
    "\n",
    "            return self.transform_image(image), l\n",
    "\n",
    "    def transform_image(self, image):\n",
    "        image = image.astype(np.float64)\n",
    "        image -= self.mean_bgr\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        image = torch.from_numpy(image.copy()).float()   ### convert from numpy array to torch tensor\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "    def get_random_crop(self, im, crop_shape):\n",
    "        \"\"\"\n",
    "        crop shape is of the format: [rows cols]\n",
    "        \"\"\"\n",
    "        r_offset = 0\n",
    "        c_offset = 0\n",
    "        \n",
    "        if im.shape[0] == crop_shape[0]:\n",
    "            r_offset = 0\n",
    "        else:\n",
    "            r_offset = np.random.randint(0, im.shape[0] - crop_shape[0] + 1)\n",
    "            \n",
    "        if im.shape[1] == crop_shape[1]:\n",
    "            c_offset = 0\n",
    "        else:\n",
    "            c_offset = np.random.randint(0, im.shape[1] - crop_shape[1] + 1)\n",
    "\n",
    "        crop_im = im[r_offset: r_offset + crop_shape[0], c_offset: c_offset + crop_shape[1], :]\n",
    "\n",
    "        return crop_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FTC3qFNJ_CUe"
   },
   "outputs": [],
   "source": [
    "#from data_loader import custom_data_loader\n",
    "\n",
    "train_list_file = cf.data_dir+'IMFDB_train.txt'   #### 5000 images for training\n",
    "val_list_file = cf.data_dir+'IMFDB_test.txt'      #### 1095 images for validation\n",
    "\n",
    "\n",
    "train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n",
    "val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n",
    "\n",
    "print(len(train_image_list), len(val_image_list))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n",
    "                                                             resize = True, resize_shape=[128,128]), \n",
    "                                          batch_size=32, num_workers=16, shuffle = True, pin_memory=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                         batch_size=10, num_workers=5, shuffle = False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-tfWqQZB_CUm"
   },
   "outputs": [],
   "source": [
    "def train_classifier(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    classifier.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #inputs, targets = Variable(inputs), Variable(targets)\n",
    "        features = feature_net(inputs)[1]      \n",
    "        \n",
    "        \n",
    "        outputs = classifier(features)\n",
    "        size_ = outputs.size()\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        loss = criterion(outputs_, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        \n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "srGe92K5_CUu"
   },
   "outputs": [],
   "source": [
    "def test_classifier(epoch):\n",
    "    global best_acc\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        #if device:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        features = feature_net.get_features(inputs).detach()\n",
    "        \n",
    "        outputs = classifier(features)\n",
    "        size_ = outputs.size()\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        loss = criterion(outputs_, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        \n",
    "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': classifier,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(cf.data_dir+'checkpoint'):\n",
    "            os.mkdir(cf.data_dir+'checkpoint')\n",
    "        torch.save(state, cf.data_dir+'checkpoint/checkpoint_ckpt.t7')\n",
    "        best_acc = acc\n",
    "    \n",
    "    return test_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bO-t7kPP_CU0"
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "experiment = 'siamese_IMFDB'\n",
    "train_loss_file = open(cf.data_dir+experiment+\"train_loss.txt\", \"rb\", 0)\n",
    "val_loss_file = open(cf.data_dir+experiment+\"val_loss.txt\", \"rb\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-5DdHCUt1XQP"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n",
    "for epoch in range(0, 30):\n",
    "    train_classifier(epoch)\n",
    "    test_loss = test_classifier(epoch)\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "train_loss_file.close()\n",
    "val_loss_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_zOf7cwQ_CVG"
   },
   "outputs": [],
   "source": [
    "Copy of def eval():\n",
    "    feature_net.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    conf_mat = np.zeros((num_classes, num_classes))\n",
    "    total_ = 1e-12+np.zeros((num_classes))\n",
    "    wrong_predictions = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        #if use_cuda:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        features = feature_net.features(inputs).detach()\n",
    "        features = features.view(1,-1)\n",
    "        #print(features.size())        \n",
    "        outputs = classifier(features)\n",
    "        size_ = outputs.size()\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        prediction = predicted.cpu().numpy()[0]\n",
    "        targets = targets.data.cpu().numpy()[0]\n",
    "        total_[targets] +=1\n",
    "        conf_mat[predicted, targets] +=1\n",
    "        \n",
    "        if prediction != targets:\n",
    "            wrong_predictions += [[inputs, prediction, targets]]\n",
    "        \n",
    "    for k in range(num_classes):\n",
    "        conf_mat[:,k] /= total_[k]\n",
    "    return conf_mat, 100.*correct/total, wrong_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xu5-2sTarNfA"
   },
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(nn.Linear(8192, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
    "                           nn.Linear(32, num_classes))\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o1Lvd4Aq1dew"
   },
   "outputs": [],
   "source": [
    "conf, acc, wrong_predictions = eval()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Uelo2wrL1gH7"
   },
   "outputs": [],
   "source": [
    "plt.imshow(conf, cmap='jet', vmin=0, vmax = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "h19bPE7C1lXK"
   },
   "outputs": [],
   "source": [
    "for w in wrong_predictions[::10]:\n",
    "    print(classes[w[2]], 'confused with', classes[w[1]])\n",
    "    plt.imshow(w[0][0][0].data.cpu().numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GVSANAgWW6WA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Experiment_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
